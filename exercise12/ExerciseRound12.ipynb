{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true;\n",
       "function code_toggle() {\n",
       "if (code_show){\n",
       "$('div.input').hide();\n",
       "} else {\n",
       "$('div.input').show();\n",
       "}\n",
       "code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#   Exercise12 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "# Preparations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import from_data_file, theta_to_model, model_to_theta, initial_model, logistic, \\\n",
    "    log_sum_exp_over_rows, classification_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 12\n",
    "\n",
    "For these exercises you will need a python environment with necessary packages installed. For installation, see the README in the main page of this assignments repository.\n",
    "\n",
    "The problems should be solved before the exercise session and solutions returned via the\n",
    "MyCourses page. Upload two files: (1) a PDF file containing your written answers to\n",
    "all problems, (2) a notebook containing the source code for the problem 3. Scanned\n",
    "handwritten solutions are ok for problem 2. Notice also that the last two problems can\n",
    "be done without solving problem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Course feedback (worth of 1 bonus point in similar manner as other tasks)\n",
    "Fill in the official course feedback through the link that you should have received to your\n",
    "email. The feedback collection is anonymous. The system separately reports the emails of\n",
    "students, who have returned the feedback, but they are not associated with the answers.\n",
    "\n",
    "__Additional material related to the following tasks is available on the MyCourses page in\n",
    "Exercise12.zip, where the slides provide some background about the concepts covered\n",
    "below. It may be helpful to check that material in case the problem context is not clear.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Neural  networks  and  backpropagation\n",
    "This is a pen-&-paper problem, see Exercise2_pen&paper.pdf for the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Image  classification  using  a  neural  network\n",
    "The exercise problem 2 above gives the solution to Part 2 of the second programming\n",
    "assignment of professor Hinton’s course “Introduction to neural networks and machine\n",
    "learning” held at the University of Toronto (https://www.cs.toronto.edu/~tijmen/csc321/ ). The assignment and related material are available at\n",
    "https://www.cs.toronto.edu/~tijmen/csc321/assignments/a2/.\n",
    "\n",
    "Check out the contents of the above web page and complete\n",
    "the programming task of Part 2 according to the instructions. The solution for the\n",
    "pen and paper part of the task is already given above. Hence, the programming part is\n",
    "a relatively straightforward implementation and can be done without carrying out the\n",
    "derivations since the results of the derivations are already given in __Exercise 2__ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient(model, data, wd_coefficient):\n",
    "    import sys\n",
    "    base_theta = model_to_theta(model)\n",
    "    h = 1e-2\n",
    "    correctness_threshold = 1e-5\n",
    "    analytic_gradient_struct = d_loss_by_d_model(model, data, wd_coefficient)\n",
    "\n",
    "    analytic_gradient = model_to_theta(analytic_gradient_struct);\n",
    "    if True in np.isnan(analytic_gradient) or True in np.isinf(analytic_gradient):\n",
    "        sys.exit('Your gradient computation produced a NaN or infinity. That is an error.')\n",
    "    # We want to test the gradient not for every element of theta, because that's a \n",
    "    # lot of work. Instead, we test for only a few elements. If there's an error, this \n",
    "    # is probably enough to find that error.\n",
    "    # We want to first test the hid_to_class gradient, because that's most likely \n",
    "    # to be correct (it's the easier one).\n",
    "    # Let's build a list of theta indices to check. We'll check 20 elements of \n",
    "    # hid_to_class, and 80 elements of input_to_hid (it's bigger than hid_to_class).\n",
    "    input_to_hid_theta_size = model['input_to_hid'].size\n",
    "    hid_to_class_theta_size = model['hid_to_class'].size\n",
    "    big_prime = 1299721; # 1299721 is prime and thus ensures a somewhat random-like selection of indices.\n",
    "    hid_to_class_indices_to_check = np.mod(big_prime * np.arange(20), hid_to_class_theta_size) \\\n",
    "                                        + input_to_hid_theta_size\n",
    "    input_to_hid_indices_to_check = np.mod(big_prime * np.arange(80), input_to_hid_theta_size)\n",
    "    a = hid_to_class_indices_to_check[np.newaxis,:]\n",
    "    b = input_to_hid_indices_to_check[np.newaxis,:]\n",
    "    indices_to_check = np.ravel(np.hstack((a,b)))\n",
    "\n",
    "    for i in range(100):\n",
    "        test_index = indices_to_check[i]\n",
    "        analytic_here = analytic_gradient[test_index]\n",
    "        theta_step = base_theta * 0\n",
    "        theta_step[test_index] = h\n",
    "        contribution_distances = np.array([-4.,  -3.,  -2.,  -1.,   1.,   2.,   3.,   4.])\n",
    "        contribution_weights = np.array([1/280., -4/105., 1/5., -4/5., 4/5., -1/5., 4/105., -1/280.])\n",
    "        temp = 0;\n",
    "        for contribution_index in range(8):\n",
    "            temp = temp + loss(theta_to_model(base_theta + theta_step * \\\n",
    "                                              contribution_distances[contribution_index]), data, wd_coefficient) * \\\n",
    "                                                contribution_weights[contribution_index]\n",
    "        fd_here = temp / h\n",
    "        diff = np.abs(analytic_here - fd_here)\n",
    "        \n",
    "        if True in (diff > correctness_threshold) and \\\n",
    "            True in (diff / (np.abs(analytic_here) + np.abs(fd_here)) > correctness_threshold):\n",
    "            part_names = ['input_to_hid', 'hid_to_class']\n",
    "            sys.exit('Theta element #{} (part of {}), with value {}, has finite difference gradient {} but analytic gradient {}. That looks like an error.\\n'.format(test_index, part_names[(i<=20)], base_theta[test_index], fd_here, analytic_here))\n",
    "        if i==20: \n",
    "            print('Gradient test passed for hid_to_class. ')\n",
    "        if i==100: \n",
    "            print('Gradient test passed for input_to_hid. ')\n",
    "    print('Gradient test passed. That means that the gradient that your code computed is within 0.001%% of the gradient that the finite difference approximation computed, so the gradient calculation procedure is probably correct (not certainly, but probably).\\n')\n",
    "    \n",
    "def forward_pass(model, data):\n",
    "    # This function does the forward pass through the network: calculating the states of all units, and some related data. \n",
    "    # This function is used in functions loss() and d_loss_by_d_model().  \n",
    "  \n",
    "    # model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>. It contains the weights from the input units to the hidden units.\n",
    "    # model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>. It contains the weights from the hidden units to the softmax units.\n",
    "    # data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>. Each column describes a different data case. \n",
    "    # data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>. Each column describes a different data case. It contains a one-of-N encoding of the class, i.e. one element in every column is 1 and the others are 0.\n",
    "    \n",
    "    hid_input = np.dot(model['input_to_hid'], data['inputs']) # input to the hidden units, i.e. before the logistic. size: <number of hidden units> by <number of data cases>\n",
    "    hid_output = logistic(hid_input) # output of the hidden units, i.e. after the logistic. size: <number of hidden units> by <number of data cases>\n",
    "    class_input = np.dot(model['hid_to_class'], hid_output) # input to the components of the softmax. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "  \n",
    "    # The following three lines of code implement the softmax.\n",
    "    # However, it's written differently from what the lectures say.\n",
    "    # In the lectures, a softmax is described using an exponential divided by a sum of exponentials.\n",
    "    # What we do here is exactly equivalent (you can check the math or just check it in practice), but this is more numerically stable. \n",
    "    # \"Numerically stable\" means that this way, there will never be really big numbers involved.\n",
    "    # The exponential in the lectures can lead to really big numbers, which are fine in mathematical equations, but can lead to all sorts of problems in Matlab\n",
    "    # Matlab isn't well prepared to deal with really large numbers, like the number 10 to the power 1000. Computations with such numbers get unstable, so we avoid them.\n",
    "\n",
    "    class_normalizer = log_sum_exp_over_rows(class_input) # log(sum(exp of class_input)) is what we subtract to get properly normalized log class probabilities. size: <1> by <number of data cases>\n",
    "    log_class_prob = class_input - np.tile(class_normalizer, (class_input.shape[0], 1)) # log of probability of each class. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "    class_prob = np.exp(log_class_prob) # probability of each class. Each column (i.e. each case) sums to 1. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "\n",
    "    return hid_input, hid_output, class_input, log_class_prob, class_prob\n",
    "\n",
    "def loss(model, data, wd_coefficient):\n",
    "    hid_input, hid_output, class_input, log_class_prob, class_prob = forward_pass(model, data);\n",
    "    classification_loss = -np.mean(np.sum(np.multiply(log_class_prob, data['target']), 0)) # select the right log class probability using that sum; then take the mean over all data cases.\n",
    "    wd_loss = (np.sum(np.ravel(model['input_to_hid']) ** 2 ) + np.sum(np.ravel(model['hid_to_class']) ** 2 )) / 2. * wd_coefficient; # weight decay loss. very straightforward: E = 1/2 * wd_coeffecient * parameters^2\n",
    "    ret = classification_loss + wd_loss\n",
    "    return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss_by_d_model(model, data, wd_coefficient):\n",
    "    # model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>\n",
    "    # model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>\n",
    "    # data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>\n",
    "    # data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>\n",
    "\n",
    "    # The returned object <ret> is supposed to be exactly like parameter <model>, i.e. it has fields ret.input_to_hid and ret.hid_to_class, and those are of the same shape as they are in <model>.\n",
    "    # However, in <ret>, the contents of those matrices are gradients (d loss by d weight), instead of weights.\n",
    "    ret = dict()\n",
    "    # This is the only function that you're expected to change. Right now, it just returns a lot of zeros, which is obviously not the correct output. Your job is to change that.\n",
    "    #--your-code-starts-here--#\n",
    "    ret['input_to_hid'] = model['input_to_hid'] * 0;\n",
    "    ret['hid_to_class'] = model['hid_to_class'] * 0;\n",
    "    #--your-code-ends-here--#\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2(wd_coefficient, n_hid, n_iters, learning_rate, momentum_multiplier, do_early_stopping, mini_batch_size):\n",
    "    model = initial_model(n_hid)\n",
    "    datas = from_data_file()\n",
    "\n",
    "    n_training_cases = datas['train']['inputs'].shape[1]\n",
    "    if n_iters != 0:\n",
    "        print(\"Now testing the gradient on the whole training set...\")\n",
    "        test_gradient(model, datas['train'], wd_coefficient)\n",
    "    \n",
    "    # optimization\n",
    "    training_batch = dict()\n",
    "    best_so_far = dict()\n",
    "    theta = model_to_theta(model)\n",
    "    momentum_speed = theta * 0.\n",
    "    training_data_losses = []\n",
    "    validation_data_losses = []\n",
    "    if do_early_stopping:\n",
    "        best_so_far['theta'] = -1 # this will be overwritten soon\n",
    "        best_so_far['validation_loss'] = np.Inf\n",
    "        best_so_far['after_n_iters'] = -1\n",
    "        \n",
    "    for optimization_iteration_i in range(1, n_iters+1):\n",
    "        model = theta_to_model(theta)\n",
    "        training_batch_start = np.mod((optimization_iteration_i-1) * mini_batch_size, n_training_cases);  \n",
    "        training_batch['inputs'] = datas['train']['inputs'][:, training_batch_start : training_batch_start + mini_batch_size]\n",
    "        training_batch['target'] = datas['train']['target'][:, training_batch_start : training_batch_start + mini_batch_size]\n",
    "        gradient = model_to_theta(d_loss_by_d_model(model, training_batch, wd_coefficient))\n",
    "        momentum_speed = np.multiply(momentum_speed, momentum_multiplier) - gradient;\n",
    "        theta = theta + momentum_speed * learning_rate;\n",
    "        model = theta_to_model(theta);\n",
    "        training_data_losses.append(loss(model, datas['train'], wd_coefficient))\n",
    "        validation_data_losses.append(loss(model, datas['val'], wd_coefficient))\n",
    "        \n",
    "        if do_early_stopping and validation_data_losses[-1] < best_so_far['validation_loss']:\n",
    "            best_so_far['theta'] = theta; # this will be overwritten soon\n",
    "            best_so_far['validation_loss'] = validation_data_losses[-1]\n",
    "            best_so_far['after_n_iters'] = optimization_iteration_i\n",
    "            \n",
    "        if np.mod(optimization_iteration_i, np.round(n_iters / 10.)) == 0:\n",
    "            print('After {} optimization iterations, training data loss is {}, and validation data loss is {}\\n'.format(optimization_iteration_i, training_data_losses[-1], validation_data_losses[-1]))\n",
    "    \n",
    "        if optimization_iteration_i == n_iters: # check gradient again, this time with more typical parameters and with a different data size\n",
    "            print('Now testing the gradient on just a mini-batch instead of the whole training set... ')\n",
    "            test_gradient(model, training_batch, wd_coefficient)\n",
    "            \n",
    "    if do_early_stopping:\n",
    "        print('Early stopping: validation loss was lowest after {} iterations. We chose the model that we had then.\\n'.format(best_so_far['after_n_iters']))\n",
    "        theta = best_so_far['theta']\n",
    "    \n",
    "    # the optimization is finished. Now do some reporting.\n",
    "    model = theta_to_model(theta)\n",
    "    if n_iters != 0:\n",
    "        ax = plt.figure(1, figsize=(15,10))\n",
    "        plt.plot(training_data_losses, 'b')\n",
    "        plt.plot(validation_data_losses, 'r')\n",
    "        plt.tick_params(labelsize=25)\n",
    "        ax.legend(('training', 'validation'), fontsize=25)\n",
    "        plt.ylabel('loss', fontsize=25);\n",
    "        plt.xlabel('iteration number', fontsize=25);\n",
    "        plt.show()\n",
    "    \n",
    "    datas2 = [datas['train'], datas['val'], datas['test']]\n",
    "    data_names = ['training', 'validation', 'test'];\n",
    "    for data_i in range(3):\n",
    "        data = datas2[data_i]\n",
    "        data_name = data_names[data_i]\n",
    "        print('\\nThe total loss on the {} data is {}\\n'.format(data_name, loss(model, data, wd_coefficient)))\n",
    "        print('The classification loss (i.e. without weight decay) on the {} data is {}\\n'.format(data_name, loss(model, data, 0)));\n",
    "        print('The classification error rate on the {} data is {}\\n'.format(data_name, classification_performance(model, data)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the neural network\n",
    "#a2(0, 10, 70, 20.0, 0, False, 4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Optimisation using backpropagation\n",
    "Do Part 3 of the assignment which is available at\n",
    "http://www.cs.toronto.edu/~tijmen/csc321/assignments/a2/\n",
    "\n",
    "The task is to experiment with the given example code and report your findings.\n",
    "There is no need to program anything in this part but completing it requires that Part 2\n",
    "is successfully solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with learning_rate = 0.002, 0.01, 0.05, 0.2, 1.0, 5.0, and 20.0, and with and without momentum_multiplier=0.9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
